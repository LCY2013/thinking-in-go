kubernetes（K8S）是什么
        Kubernetes（常简称为K8S）是用于自动部署、扩展和管理容器化（containerized）应用程序的开源系统。
    系统由Google设计并捐赠给Cloud Native Computing Foundation（今属Linux基金会）来使用。
        它旨在提供“跨主机集群的自动部署、扩展以及运行应用程序容器的平台”。它支持一系列容器工具, 包括Docker等。
    我们可以将Docker看成Kubernetes内部使用的低级别组件。

为什么要使用kubernetes
        Docker这个新兴的容器化技术当前应用越来越广，并且其从单机走向集群也称为必然，
    而云计算的蓬勃发展正在加速这一进程。kubernetes作为当前普遍被业界广泛认可和看好的docker分布式系统解决方案，前景非常可观。

使用Kubernetes可以做什么：
    自动化容器的部署和复制
    随时扩展或收缩容器规模
    将容器组织成组，并且提供容器间的负载均衡
    很容易地升级应用程序容器的新版本
    提供容器弹性，如果容器失效就替换它
    部署环境准备
    机器信息

#修改主机名称
#master节点:
hostnamectl set-hostname k8sm-120
#node1节点：
hostnamectl set-hostname k8s-121
#node2节点:
hostnamectl set-hostname k8s-122

修改hosts
192.168.99.120 k8sm-120
192.168.99.121 k8s-121
192.168.99.122 k8s-122

增加网络转发
# cat <<EOF >  /etc/sysctl.d/k8s.conf
vm.swappiness = 0
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
使配置生效
# modprobe br_netfilter
# sysctl -p /etc/sysctl.d/k8s.conf

#关闭防火墙和selinux
systemctl stop firewalld & systemctl disable firewalld
sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config && setenforce 0

关闭swap
# swapoff -a
# yes | cp /etc/fstab /etc/fstab_bak
# cat /etc/fstab_bak |grep -v swap > /etc/fstab

配置时间同步
使用chrony同步时间，配置master节点与网络NTP服务器同步时间，所有node节点与master节点同步时间。
配置master节点
    安装chrony：
    # yum install -y chrony
    注释默认ntp服务器
    # sed -i 's/^server/#&/' /etc/chrony.conf
    指定上游公共 ntp 服务器，并允许其他节点同步时间
# cat >> /etc/chrony.conf << EOF
server 0.asia.pool.ntp.org iburst
server 1.asia.pool.ntp.org iburst
server 2.asia.pool.ntp.org iburst
server 3.asia.pool.ntp.org iburst
allow all
EOF
    重启chronyd服务并设为开机启动：
    # systemctl enable chronyd && systemctl restart chronyd
    开启网络时间同步功能
    # timedatectl set-ntp true
    更改当前时区为东8区【统一时区，亚洲上海】
    # timedatectl set-timezone Asia/Shanghai
配置所有node节点：
    (注意修改master IP地址)
    安装chrony：
    # yum install -y chrony
    注释默认服务器
    # sed -i 's/^server/#&/' /etc/chrony.conf
    指定内网 master节点为上游NTP服务器
    # echo server 192.168.99.120 iburst >> /etc/chrony.conf
    重启服务并设为开机启动：
    # systemctl enable chronyd && systemctl restart chronyd
    结果为^*则同步完成
    # chronyc sources -v
所有节点执行chronyc sources命令，查看存在以^*开头的行，说明已经与服务器时间同步    

#加载ipvs相关模块
由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块：
在所有的Kubernetes节点执行以下脚本:
# cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
#执行脚本
# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
上面脚本创建了/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 
使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。
接下来还需要确保各个节点上已经安装了ipset软件包。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm。
# yum install ipset ipvsadm -y


安装组件
安装指定版本Docker
# wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# yum install docker-ce-18.06.1.ce-3.el7 -y
# systemctl start docker 
# systemctl enable docker 
# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

使用阿里云镜像安装指定版本kubelet，kubeadm，kubectl
# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
在所有节点上安装指定版本 kubelet、kubeadm 和 kubectl
# yum install -y kubelet-1.19.0 kubeadm-1.19.0 kubectl-1.19.0
启动kubelet服务
# systemctl enable kubelet && systemctl start kubelet

完整的官方文档可以参考：
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/
初始化master
# kubeadm init --kubernetes-version=v1.19.0 \
    --apiserver-advertise-address=192.168.99.120 \
    --image-repository registry.aliyuncs.com/google_containers \
    --pod-network-cidr=10.244.0.0/16 \
    --service-cidr=10.96.0.0/12

初始化命令说明：
--apiserver-advertise-address
指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个 interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的 interface。
--pod-network-cidr
指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对 --pod-network-cidr 有自己的要求，这里设置为 10.244.0.0/16 是因为我们将使用 flannel 网络方案，必须设置成这个 CIDR。
--image-repository
Kubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问 gcr.io，在1.13版本中我们可以增加–image-repository参数，默认值是 k8s.gcr.io，将其指定为阿里云镜像地址：registry.aliyuncs.com/google_containers。
--kubernetes-version=v1.19.0
关闭版本探测，因为它的默认值是stable-1，会导致从https://dl.k8s.io/release/stable-1.txt下载最新的版本号，我们可以将其指定为固定版本（最新版：v1.13.1）来跳过网络请求。
--service-cidr：指定service网段,负载均衡ip
--ignore-preflight-errors=Swap/all：忽略 swap/所有 报错
初始化成功结果
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.99.120:6443 --token iqvttf.bh82wt455vecnkx0 \
    --discovery-token-ca-cert-hash sha256:dc44a71b5bb598f60eb16eef40ce6930022709b85a209961d5984d8f9e1f0c3b
初始化过程说明：
[preflight] kubeadm 执行初始化前的检查。
[kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”
[certificates] 生成相关的各种token和证书
[kubeconfig] 生成 KubeConfig 文件，kubelet 需要这个文件与 Master 通信
[control-plane] 安装 Master 组件，会从指定的 Registry 下载组件的 Docker 镜像。
[bootstraptoken] 生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
[addons] 安装附加组件 kube-proxy 和 kube-dns。
Kubernetes Master 初始化成功，提示如何配置常规用户使用kubectl访问集群。
提示如何安装 Pod 网络。
提示如何注册其他节点到 Cluster。

配置 kubectl
kubectl 是管理 Kubernetes Cluster 的命令行工具，前面我们已经在所有的节点安装了 kubectl。Master 初始化完成后需要做一些配置工作，然后 kubectl 就能使用了。
依照 kubeadm init 输出的最后提示，推荐用 Linux 普通用户执行 kubectl。
增加kubectl权限访问
此处为初始化成功输出的内容，复制即可
# mkdir -p $HOME/.kube
# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
# sudo chown $(id -u):$(id -g) $HOME/.kube/config
启用 kubectl 命令自动补全功能（注销重新登录生效）
# echo "source <(kubectl completion bash)" >> ~/.bashrc
需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。
如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。
配置完成后centos用户就可以使用 kubectl 命令管理集群了。

部署pod网络插件
要让 Kubernetes Cluster 能够工作，必须安装 Pod 网络，否则 Pod 之间无法通信。
Kubernetes 支持多种网络方案，这里我们使用 flannel
# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
# kubectl get pod -n kube-system -o wide
可以看到，所有的系统 Pod 都成功启动了，而刚刚部署的flannel网络插件则在 kube-system 下面新建了一个名叫kube-flannel-ds-amd64-lkf2f的 Pod，
一般来说，这些 Pod 就是容器网络插件在每个节点上的控制组件。
Kubernetes 支持容器网络插件，使用的是一个名叫 CNI 的通用接口，它也是当前容器网络的事实标准，
市面上的所有容器网络开源项目都可以通过 CNI 接入 Kubernetes，比如 Flannel、Calico、Canal、Romana 等等，
它们的部署方式也都是类似的“一键部署”。
# kubectl get nodes
至此，Kubernetes 的 Master 节点就部署完成了。如果你只需要一个单节点的 Kubernetes，现在你就可以使用了。
不过，在默认情况下，Kubernetes 的 Master 节点是不能运行用户 Pod 的。

部署worker节点
Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。
唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 
kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。
增加node到集群里
# kubeadm join 192.168.99.120:6443 --token iqvttf.bh82wt455vecnkx0 \
      --discovery-token-ca-cert-hash sha256:dc44a71b5bb598f60eb16eef40ce6930022709b85a209961d5984d8f9e1f0c3b
如果执行kubeadm init时没有记录下加入集群的命令，可以通过以下命令重新创建
# kubeadm token create --print-join-command

master节点查看node状态
# kubectl get node
NAME       STATUS   ROLES    AGE   VERSION
k8s-121    Ready    <none>   34m   v1.19.0
k8s-122    Ready    <none>   34m   v1.19.0
k8sm-120   Ready    master   40m   v1.19.0
nodes状态全部为ready，由于每个节点都需要启动若干组件，如果node节点的状态是 NotReady，
可以查看所有节点pod状态，确保所有pod成功拉取到镜像并处于running状态
这时，所有的节点都已经 Ready，Kubernetes Cluster 创建成功，一切准备就绪。
如果pod状态为Pending、ContainerCreating、ImagePullBackOff 都表明 Pod 没有就绪，Running 才是就绪状态。
如果有pod提示Init:ImagePullBackOff，说明这个pod的镜像在对应节点上拉取失败，我们可以通过 kubectl describe pod 查看 Pod 具体情况，以确认拉取失败的镜像
# kubectl describe pod kube-flannel-ds-amd64-scf6b --namespace=kube-system
这里看最后events输出内容，可以看到在下载 image 时失败，如果网络质量不好，这种情况是很常见的。
我们可以耐心等待，因为 Kubernetes 会重试，我们也可以自己手工执行 docker pull 去下载这个镜像
# docker pull quay.io/coreos/flannel:v0.12.0-amd64
如果无法从 quay.io/coreos/flannel:v0.10.0-amd64 下载镜像，可以从阿里云或者dockerhub镜像仓库下载，然后改回原来的tag即可：
# docker pull registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/flannel:v0.12.0-amd64
# docker tag registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/flannel:v0.12.0-amd64 quay.io/coreos/flannel:v0.12.0-amd64
# docker rmi registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/flannel:v0.12.0-amd64
查看master节点下载了哪些镜像
# docker images
查看node节点下载了哪些镜像：
# docker images

测试集群各个组件
首先验证kube-apiserver, kube-controller-manager, kube-scheduler, pod network 是否正常：
部署一个 Nginx Deployment，包含2个Pod
参考：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
# kubectl create deployment nginx --image=nginx:alpine
# kubectl scale deployment nginx --replicas=2
验证Nginx Pod是否正确运行，并且会分配10.244.开头的集群IP
# kubectl get pods -l app=nginx -o wide
再验证一下kube-proxy是否正常：
以 NodePort 方式对外提供服务
参考：https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/
# kubectl expose deployment nginx --port=80 --type=NodePort
# kubectl get services nginx
最后验证一下dns, pod network是否正常：
运行Busybox并进入交互模式
# kubectl run -it curl --image=radial/busyboxplus:curl
输入nslookup nginx查看是否可以正确解析出集群内的IP，以验证DNS是否正常
# curl http://nginx/
分别访问一下2个Pod的内网IP，验证跨Node的网络通信是否正常
# curl 10.244.2.3
# curl 10.244.1.3

Pod调度到Master节点
出于安全考虑，默认配置下Kubernetes不会将Pod调度到Master节点。查看Taints字段默认配置：
# kubectl describe node k8sm-120

部署存储插件rook
$ kubectl taint node k8sm-120 node-role.kubernetes.io/master="":NoSchedule
$ kubectl label nodes {k8s-121,k8s-122} ceph-osd=enabled
$ kubectl label nodes {k8s-121,k8s-122} ceph-mon=enabled
提示：当前版本rook中mgr只能支持一个节点运行
$ kubectl label nodes k8s-121 ceph-mgr=enabled

地址: https://github.com/rook/rook 可以在这里手动下载operator.yaml、cluster.yaml、common.yaml
$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml
$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
文档 参考<cluster.yaml>
$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml
查看rook相关pod
$ kubectl get pods -n rook-ceph
强制删除 namespace  rook-ceph
$ kubectl get namespace rook-ceph -o json \
            | tr -d "\n" | sed "s/\"finalizers\": \[[^]]\+\]/\"finalizers\": []/" \
            | kubectl replace --raw /api/v1/namespaces/rook-ceph/finalize -f -


部署webui dashboard
地址: https://github.com/kubernetes/dashboard
下载官方的yaml文件:
# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml
具体搭建可以查看: https://www.cnblogs.com/yy690486439/p/13597400.html
------- kubernetes-dashboard 2.x 版本安装.txt


-----卸载k8s
Clean up
If you used disposable servers for your cluster, for testing, you can switch those off and do no further clean up. You can use kubectl config delete-cluster to delete your local references to the cluster.
However, if you want to deprovision your cluster more cleanly, you should first drain the node and make sure that the node is empty, then deconfigure the node.
Remove the node
Talking to the control-plane node with the appropriate credentials, run:
# kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
Before removing the node, reset the state installed by kubeadm:
# kubeadm reset
The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:
# iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
If you want to reset the IPVS tables, you must run the following command:
# ipvsadm -C
Now remove the node:
# kubectl delete node <node name>

首先清理运行到k8s群集中的pod，使用
# kubectl delete node --all
然后从主机系统中删除数据卷和备份（如果不需要）。最后，可以使用脚本停止所有k8s服务，
# for service in kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler; do
      systemctl stop $service
  done
删除rpm包
# rpm -qa|grep kube*|xargs rpm --nodeps -e
删除容器及镜像
# docker images -qa|xargs docker rmi -f
# docker ps -qa|xargs docker rm -f
# rm -rf /etc/kubernetes
删除相关k8s信息
# find / -name ku*
检查 docker 的日志也可能有用：
# journalctl -ul docker


在卸载K8s组件前，先执行kubeadm reset命令，清空K8s集群设置
# kubeadm reset
卸载管理组件
# yum erase -y kubelet kubectl kubeadm kubernetes-cni

# netstat -nltp | grep kube

# kubectl get cs

# journalctl -u kube-controller-manager --since '2020-09-03 09:33:00'

# kubectl create clusterrolebinding system-node-role-bound --clusterrole=system:kube-admin --group=cluster

# netstat -lnpt|grep kube


-------- 基于kubeadm安装集群踩坑记录 ----------
https://www.jianshu.com/p/ed1ae8443fff
重新安装后会存在kube-controller-manager和kube-scheduler启动端口和api-server监听的端口不一致的情况
需要修改如下文件: 基本目录(/etc/kubernetes)
    1、/etc/kubernetes/manifests/kube-controller-manager.yaml
        注释掉--port=0 关闭监听非安全端口（http），同时 --address 参数无效，--bind-address 参数有效；
        修改正确的端口号10252
    2、/etc/kubernetes/manifests/kube-scheduler.yaml
        注释掉--port=0 关闭监听非安全端口（http），同时 --address 参数无效，--bind-address 参数有效；
        修改正确的端口号10251



POD 无法跨界点通信:
    node上分别执行
    # iptables -P INPUT ACCEPT
    # iptables -P FORWARD ACCEPT
    # iptables -F
    # iptables -L -n

    采用kubeadm安装的时候，使用的命令并非默认命令
        # kubeadm init --pod-network-cidr=10.222.0.0/16
        K8S调度分发到每个NODE的时候分配的子网并不是默认的 10.244.0.0/16
        # kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
        删除flannel组件
        # kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
        将yml文件下载下来
        # wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
        编辑yml中的net-conf.json字段，使其中的子网分配为你kubeadm初始化的子网地址
        重新应用kubectl apply -f kube-flannel.yml

















