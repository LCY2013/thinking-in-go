### [CSI 插件编写](https://github.com/digitalocean/csi-digitalocean)
CSI 插件之后，持久化存储的用法就变得简单了，只需要创建一个如下所示的StorageClass 对象即可：
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: do-block-storage
  namespace: kube-system
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: com.digitalocean.csi.dobs
```
有了这个 StorageClass，External Provisoner 就会为集群中新出现的 PVC 自动创建出PV，然后调用 CSI 插件创建出这个 PV 对应的 Volume，这正是 CSI 体系中 DynamicProvisioning 的实现方式。

storageclass.kubernetes.io/is-default-class: "true"的意思，是使用这个 StorageClass 作为默认的持久化存储提供者。

不难看到，这个 StorageClass 里唯一引人注意的，是provisioner=com.digitalocean.csi.dobs 这个字段。显然，这个字段告诉了Kubernetes，请使用名叫 com.digitalocean.csi.dobs 的 CSI 插件来处理这个StorageClass 相关的所有操作。

#### CSI Identity

Kubernetes 又是如何知道一个 CSI 插件的名字的呢？

这就需要从 CSI 插件的第一个服务 CSI Identity 说起了。

其实，一个 CSI 插件的代码结构非常简单，如下所示：
```text
tree $GOPATH/src/github.com/digitalocean/csi-digitalocean/driver  
$GOPATH/src/github.com/digitalocean/csi-digitalocean/driver 
├── controller.go
├── driver.go
├── identity.go
├── mounter.go
└── node.go
```
其中，CSI Identity 服务的实现，就定义在了 driver 目录下的 identity.go 文件里。

为了让 Kubernetes 访问到 CSI Identity 服务，需要先在 driver.go 文件里，定义一个标准的 gRPC Server，如下所示：
```text
// Run starts the CSI plugin by communication over the given endpoint
func (d *Driver) Run(ctx context.Context) error {
	...

	grpcListener, err := net.Listen(u.Scheme, grpcAddr)
	
	...
	
	d.srv = grpc.NewServer(grpc.UnaryInterceptor(errHandler))
	csi.RegisterIdentityServer(d.srv, d)
	csi.RegisterControllerServer(d.srv, d)
	csi.RegisterNodeServer(d.srv, d)

	d.ready = true // we're now ready to go!
	...
	return eg.Wait()
}
```
可以看到，只要把编写好的 gRPC Server 注册给 CSI，它就可以响应来自 ExternalComponents 的 CSI 请求了。

[CSI Identity](https://github.com/container-storage-interface/spec/blob/master/csi.proto) 服务中，最重要的接口是 GetPluginInfo，它返回的就是这个插件的名字和版本号，如下所示：
```text
// GetPluginInfo returns metadata of the plugin
func (d *Driver) GetPluginInfo(ctx context.Context, req *csi.GetPluginInfoRequest) (*csi.GetPluginInfoResponse, error) {
	resp := &csi.GetPluginInfoResponse{
		Name:          d.name,
		VendorVersion: version,
	}

	d.log.WithFields(logrus.Fields{
		"response": resp,
		"method":   "get_plugin_info",
	}).Info("get plugin info called")
	return resp, nil
}
```
其中，d.name 的值，正是"com.digitalocean.csi.dobs"。所以说，Kubernetes 正是通过 GetPluginInfo 的返回值，来找到在 StorageClass 里声明要使用的 CSI 插件的。

CSI 要求插件的名字遵守“反向 DNS”格式。

另外一个GetPluginCapabilities 接口也很重要，这个接口返回的是这个 CSI 插件的“能力”。
```text
// GetPluginCapabilities returns available capabilities of the plugin
func (d *Driver) GetPluginCapabilities(ctx context.Context, req *csi.GetPluginCapabilitiesRequest) (*csi.GetPluginCapabilitiesResponse, error) {
	resp := &csi.GetPluginCapabilitiesResponse{
		Capabilities: []*csi.PluginCapability{
			{
				Type: &csi.PluginCapability_Service_{
					Service: &csi.PluginCapability_Service{
						Type: csi.PluginCapability_Service_CONTROLLER_SERVICE,
					},
				},
			},
			{
				Type: &csi.PluginCapability_Service_{
					Service: &csi.PluginCapability_Service{
						Type: csi.PluginCapability_Service_VOLUME_ACCESSIBILITY_CONSTRAINTS,
					},
				},
			},
			{
				Type: &csi.PluginCapability_VolumeExpansion_{
					VolumeExpansion: &csi.PluginCapability_VolumeExpansion{
						Type: csi.PluginCapability_VolumeExpansion_ONLINE,
					},
				},
			},
		},
	}
    ...
	return resp, nil
}
```
比如，当你编写的 CSI 插件不准备实现“Provision 阶段”和“Attach 阶段”（比如，一个最简单的 NFS 存储插件就不需要这两个阶段）时，你就可以通过这个接口返回：本插件不提供 CSI Controller 服务，即：没有csi.PluginCapability_Service_CONTROLLER_SERVICE 这个“能力”。这样，Kubernetes 就知道这个信息了。

最后，CSI Identity 服务还提供了一个 Probe 接口。Kubernetes 会调用它来检查这个CSI 插件是否正常工作。
```text
// Probe returns the health and readiness of the plugin
func (d *Driver) Probe(ctx context.Context, req *csi.ProbeRequest) (*csi.ProbeResponse, error) {
	d.readyMu.Lock()
	defer d.readyMu.Unlock()

	return &csi.ProbeResponse{
		Ready: &wrappers.BoolValue{
			Value: d.ready,
		},
	}, nil
}
```
一般情况下，建议在编写插件时给它设置一个 Ready 标志，当插件的 gRPC Server 停止的时候，把这个 Ready 标志设置为 false。或者，可以在这里访问一下插件的端口，类似于健康检查的做法。

#### CSI Controller
开始编写 CSI 插件的第二个服务，即 CSI Controller 服务了。它的代码实现，在 controller.go 文件里。

这个服务主要实现的就是 Volume 管理流程中的“Provision 阶段”和“Attach 阶段”。

”Provision 阶段”对应的接口，是 CreateVolume 和 DeleteVolume，它们的调用者是 External Provisoner。以 CreateVolume 为例，主要逻辑如下所示：
```text
// CreateVolume creates a new volume from the given request. The function is
// idempotent.
func (d *Driver) CreateVolume(ctx context.Context, req *csi.CreateVolumeRequest) (*csi.CreateVolumeResponse, error) {
	...
	volumeReq := &godo.VolumeCreateRequest{
		Region:        d.region,
		Name:          volumeName,
		Description:   createdByDO,
		SizeGigaBytes: size / giB,
	}
	...
	resp := &csi.CreateVolumeResponse{
		Volume: &csi.Volume{
			VolumeId:      vol.ID,
			CapacityBytes: size,
			AccessibleTopology: []*csi.Topology{
				{
					Segments: map[string]string{
						"region": d.region,
					},
				},
			},
		},
	}
	return resp, nil
}
```
可以看到，对于 DigitalOcean 这样的公有云来说，CreateVolume 需要做的操作，就是调用 DigitalOcean 块存储服务的 API，创建出一个存储卷（d.doClient.Storage.CreateVolume）。如果使用的是其他类型的块存储（比如Cinder、Ceph RBD 等），对应的操作也是类似地调用创建存储卷的 API。

而“Attach 阶段”对应的接口是 ControllerPublishVolume 和ControllerUnpublishVolume，调用者是 External Attacher。以ControllerPublishVolume 为例，逻辑如下所示：
```text
// ControllerPublishVolume attaches the given volume to the node
func (d *Driver) ControllerPublishVolume(ctx context.Context, req *csi.ControllerPublishVolumeRequest) (*csi.ControllerPublishVolumeResponse, error) {
	...
	dropletID, err := strconv.Atoi(req.NodeId)
    ...
	// check if volume exist before trying to attach it
	vol, resp, err := d.storage.GetVolume(ctx, req.VolumeId)
	...
	// check if droplet exist before trying to attach the volume to the droplet
	_, resp, err = d.droplets.Get(ctx, dropletID)
	...
	// attach the volume to the correct node
	action, resp, err := d.storageActions.Attach(ctx, req.VolumeId, dropletID)
	...
	if action != nil {
		log.Info("waiting until volume is attached")
		if err := d.waitAction(ctx, log, req.VolumeId, action.ID); err != nil {
			return nil, err
		}
	}

	return &csi.ControllerPublishVolumeResponse{
		PublishContext: map[string]string{
			d.publishInfoVolumeName: vol.Name,
		},
	}, nil
}
```
可以看到，对于 DigitalOcean 来说，ControllerPublishVolume 在“Attach 阶段”需要做的工作，是调用 DigitalOcean 的 API，将前面创建的存储卷，挂载到指定的虚拟机上（d.doClient.StorageActions.Attach）。

其中，存储卷由请求中的 VolumeId 来指定。而虚拟机，也就是将要运行 Pod 的宿主机，则由请求中的 NodeId 来指定。这些参数，都是 External Attacher 在发起请求时需要设置的。

External Attacher 的工作原理，是监听（Watch）了一种名叫 VolumeAttachment 的 API 对象。这种 API 对象的主要字段如下所示：
```text
// VolumeAttachmentSpec is the specification of a VolumeAttachment request.
type VolumeAttachmentSpec struct { 
    // Attacher indicates the name of the volume driver that MUST handle this 
    // request. This is the name returned by GetPluginName(). 
    Attacher string 
    // Source represents the volume that should be attached. 
    Source VolumeAttachmentSource 
    // The node that the volume should be attached to. 
    NodeName string
}
```
这个控制循环的职责，是不断检查 Pod 所对应的 PV，在它所绑定的宿主机上的挂载情况，从而决定是否需要对这个 PV 进行 Attach（或者 Dettach）操作。

而这个 Attach 操作，在 CSI 体系里，就是创建出上面这样一个 VolumeAttachment 对象。可以看到，Attach 操作所需的 PV 的名字（Source）、宿主机的名字（NodeName）、存储插件的名字（Attacher），都是这个 VolumeAttachment 对象的一部分。

而当 External Attacher 监听到这样的一个对象出现之后，就可以立即使用VolumeAttachment 里的这些字段，封装成一个 gRPC 请求调用 CSI Controller 的ControllerPublishVolume 方法。

### CSI Node 服务
CSI Node 服务对应的，是 Volume 管理流程里的“Mount 阶段”。[代码位置](https://github.com/digitalocean/csi-digitalocean/blob/master/driver/node.go) 。

kubelet 的 VolumeManagerReconciler 控制循环会直接调用 CSI Node 服务来完成 Volume 的“Mount 阶段”。

不过，在具体的实现中，这个“Mount 阶段”的处理其实被细分成了 NodeStageVolume和 NodePublishVolume 这两个接口。

在 kubelet 的 VolumeManagerReconciler 控制循环中，这两步操作分别叫作MountDevice 和 SetUp。

















